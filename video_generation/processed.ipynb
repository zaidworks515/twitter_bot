{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"D:\\Tessaract\\Projects API\\twitter bot\\twitter_bot\\video generation\\ffmpeg\\bin\"\n",
    "\n",
    "def augment_video(video_path, times):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "    out = cv2.VideoWriter(\"first_output.mp4\", fourcc, fps, (width, height))\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) < 7:  \n",
    "        print(\"Not enough frames to process smoothly.\")\n",
    "        return\n",
    "\n",
    "    frames = frames[5:]\n",
    "\n",
    "    sequence = []\n",
    "    for i in range(times):\n",
    "        if i % 2 == 0:\n",
    "            sequence.extend(frames)  \n",
    "        else:\n",
    "            sequence.extend(frames[::-1])\n",
    "\n",
    "    for frame in sequence:\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Video saved as first_output.mp4 with {times} sequence(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import VideoFileClip, AudioFileClip \n",
    "import math\n",
    "\n",
    "\n",
    "def increase_template_video_length(audio_path, video_path):\n",
    "    \n",
    "    audio = AudioFileClip(audio_path)\n",
    "    video = VideoFileClip(video_path)\n",
    "    \n",
    "    audio_length = audio.duration\n",
    "    video_length = video.duration\n",
    "    print(audio_length)\n",
    "    \n",
    "    # NOW, MAKING THE VIDEO EQUAL TO THE AUDIO LENGTH....\n",
    "    \n",
    "    if audio_length > video_length:\n",
    "        max_video_length = math.ceil(audio_length)\n",
    "        \n",
    "        augment_video(video_path, max_video_length)\n",
    "        \n",
    "        second_video = VideoFileClip('first_output.mp4')\n",
    "        second_video.audio = audio.subclipped(0, audio.duration)\n",
    "\n",
    "        trimmed_video = second_video.subclipped(0, audio_length)\n",
    "\n",
    "\n",
    "        trimmed_video.write_videofile(\"trimmed_output.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n",
    "        \n",
    "\n",
    "\n",
    "video_path = \"avatar_forward.mp4\"\n",
    "audio_path = \"sample.mp3\"\n",
    "\n",
    "        \n",
    "increase_template_video_length(audio_path, video_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import moviepy as mp\n",
    "import whisper\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"D:\\Tessaract\\Projects API\\twitter bot\\twitter_bot\\video_generation\\ffmpeg\\bin\"\n",
    "\n",
    "def overlay_subtitles(video_path, output_path, subtitle_chunks):\n",
    "    video = mp.VideoFileClip(video_path)\n",
    "\n",
    " \n",
    "    def add_overlay(frame):\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "        current_time = video.reader.pos / video.fps  # Accurate frame time\n",
    "        current_text = \"\"\n",
    "        for chunk in subtitle_chunks:\n",
    "            if chunk['start'] <= current_time <= chunk['end']:\n",
    "                current_text = chunk['text']\n",
    "                break\n",
    "\n",
    "        if current_text:\n",
    "            font_scale = 1\n",
    "            font_thickness = 2\n",
    "            text_color = (255, 255, 255)  # White text\n",
    "            shadow_color = (0, 0, 0)  # Black shadow\n",
    "            font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "\n",
    "            text_size, _ = cv2.getTextSize(current_text, font, font_scale, font_thickness)\n",
    "            text_x = (video.w - text_size[0]) // 2\n",
    "            text_y = video.h - 50  # Position at the bottom center\n",
    "\n",
    "            # Shadow effect\n",
    "            cv2.putText(frame_bgr, current_text, (text_x + 2, text_y + 2), font, font_scale, shadow_color, font_thickness + 1, cv2.LINE_AA)\n",
    "            cv2.putText(frame_bgr, current_text, (text_x, text_y), font, font_scale, text_color, font_thickness, cv2.LINE_AA)\n",
    "\n",
    "        return cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)\n",
    "\n",
    "    modified_video = video.image_transform(add_overlay)\n",
    "    modified_video.write_videofile(output_path, codec=\"libx264\", fps=video.fps, audio_codec=\"aac\")\n",
    "\n",
    "\n",
    "\n",
    "def extract_subtitles(audio_path, shift_time=-0.1):\n",
    "    model = whisper.load_model(\"small\")\n",
    "    result = model.transcribe(audio_path)\n",
    "    subtitles = []\n",
    "\n",
    "    for segment in result[\"segments\"]:\n",
    "        start_time = max(0, segment[\"start\"] + shift_time)\n",
    "        end_time = segment[\"end\"] + shift_time\n",
    "        text = segment[\"text\"]\n",
    "\n",
    "        mid_time = (start_time + end_time) / 2  \n",
    "        words = text.split()\n",
    "\n",
    "        if len(words) > 1:\n",
    "            mid_index = len(words) // 2  \n",
    "            first_half = \" \".join(words[:mid_index])\n",
    "            second_half = \" \".join(words[mid_index:])\n",
    "\n",
    "            subtitles.append({\"start\": start_time, \"end\": mid_time, \"text\": first_half})\n",
    "            subtitles.append({\"start\": mid_time, \"end\": end_time, \"text\": second_half})\n",
    "        else:\n",
    "            subtitles.append({\"start\": start_time, \"end\": end_time, \"text\": text})\n",
    "\n",
    "    return subtitles\n",
    "\n",
    "\n",
    "video_path = \"trimmed_output.mp4\"  \n",
    "audio_path = \"sample.mp3\"\n",
    "output_video_path = \"final_output.mp4\"\n",
    "\n",
    "\n",
    "subtitles = extract_subtitles(audio_path)\n",
    "overlay_subtitles(video_path, output_video_path, subtitles)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from moviepy import VideoFileClip, AudioFileClip, CompositeAudioClip\n",
    "\n",
    "def add_background_music(video_path, music_path, output_path, music_volume=0.05):\n",
    "    video = VideoFileClip(video_path)\n",
    "\n",
    "    # Load the background music and set volume to 20%\n",
    "    bg_music = AudioFileClip(music_path).with_volume_scaled(music_volume)\n",
    "\n",
    "    # Set music duration to match the video length\n",
    "    bg_music = bg_music.with_duration(video.duration)\n",
    "\n",
    "    # Combine the video audio and background music\n",
    "    final_audio = CompositeAudioClip([video.audio, bg_music])\n",
    "\n",
    "    # Set the final audio to the video\n",
    "    final_video = video.with_audio(final_audio)\n",
    "\n",
    "    # Export the final video\n",
    "    final_video.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "\n",
    "# Example usage\n",
    "video_path = \"final_output.mp4\"\n",
    "music_path = \"bg_voice_basketball.mp3\"\n",
    "output_path = \"final_video_with_music.mp4\"\n",
    "\n",
    "add_background_music(video_path, music_path, output_path)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting moviepy==1.0.3\n",
      "  Downloading moviepy-1.0.3.tar.gz (388 kB)\n",
      "     -------------------------------------- 388.3/388.3 kB 1.2 MB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Collecting decorator<5.0,>=4.0.2\n",
      "  Downloading decorator-4.4.2-py2.py3-none-any.whl (9.2 kB)\n",
      "Requirement already satisfied: tqdm<5.0,>=4.11.2 in d:\\tessaract\\projects api\\twitter bot\\venv\\lib\\site-packages (from moviepy==1.0.3) (4.67.1)\n",
      "Requirement already satisfied: requests<3.0,>=2.8.1 in d:\\tessaract\\projects api\\twitter bot\\venv\\lib\\site-packages (from moviepy==1.0.3) (2.32.3)\n",
      "Requirement already satisfied: proglog<=1.0.0 in d:\\tessaract\\projects api\\twitter bot\\venv\\lib\\site-packages (from moviepy==1.0.3) (0.1.10)\n",
      "Requirement already satisfied: numpy>=1.17.3 in d:\\tessaract\\projects api\\twitter bot\\venv\\lib\\site-packages (from moviepy==1.0.3) (1.26.4)\n",
      "Requirement already satisfied: imageio<3.0,>=2.5 in d:\\tessaract\\projects api\\twitter bot\\venv\\lib\\site-packages (from moviepy==1.0.3) (2.37.0)\n",
      "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in d:\\tessaract\\projects api\\twitter bot\\venv\\lib\\site-packages (from moviepy==1.0.3) (0.6.0)\n",
      "Requirement already satisfied: pillow>=8.3.2 in d:\\tessaract\\projects api\\twitter bot\\venv\\lib\\site-packages (from imageio<3.0,>=2.5->moviepy==1.0.3) (10.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in d:\\tessaract\\projects api\\twitter bot\\venv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (2024.12.14)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in d:\\tessaract\\projects api\\twitter bot\\venv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (2.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\tessaract\\projects api\\twitter bot\\venv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in d:\\tessaract\\projects api\\twitter bot\\venv\\lib\\site-packages (from requests<3.0,>=2.8.1->moviepy==1.0.3) (3.10)\n",
      "Requirement already satisfied: colorama in d:\\tessaract\\projects api\\twitter bot\\venv\\lib\\site-packages (from tqdm<5.0,>=4.11.2->moviepy==1.0.3) (0.4.6)\n",
      "Using legacy 'setup.py install' for moviepy, since package 'wheel' is not installed.\n",
      "Installing collected packages: decorator, moviepy\n",
      "  Attempting uninstall: decorator\n",
      "    Found existing installation: decorator 5.1.1\n",
      "    Uninstalling decorator-5.1.1:\n",
      "      Successfully uninstalled decorator-5.1.1\n",
      "  Attempting uninstall: moviepy\n",
      "    Found existing installation: moviepy 2.1.2\n",
      "    Uninstalling moviepy-2.1.2:\n",
      "      Successfully uninstalled moviepy-2.1.2\n",
      "  Running setup.py install for moviepy: started\n",
      "  Running setup.py install for moviepy: finished with status 'done'\n",
      "Successfully installed decorator-4.4.2 moviepy-1.0.3\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install moviepy==1.0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.11.0.86-cp37-abi3-win_amd64.whl (39.5 MB)\n",
      "     ---------------------------------------- 39.5/39.5 MB 3.7 MB/s eta 0:00:00\n",
      "Collecting opencv-contrib-python\n",
      "  Downloading opencv_contrib_python-4.11.0.86-cp37-abi3-win_amd64.whl (46.2 MB)\n",
      "     ---------------------------------------- 46.2/46.2 MB 3.3 MB/s eta 0:00:00\n",
      "Collecting opencv-contrib-python-headless\n",
      "  Downloading opencv_contrib_python_headless-4.11.0.86-cp37-abi3-win_amd64.whl (46.1 MB)\n",
      "     ---------------------------------------- 46.1/46.1 MB 2.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy>=1.21.2 in d:\\tessaract\\projects api\\twitter bot\\venv\\lib\\site-packages (from opencv-python) (1.26.4)\n",
      "Installing collected packages: opencv-python, opencv-contrib-python-headless, opencv-contrib-python\n",
      "Successfully installed opencv-contrib-python-4.11.0.86 opencv-contrib-python-headless-4.11.0.86 opencv-python-4.11.0.86\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install opencv-python opencv-contrib-python opencv-contrib-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ffmpeg-python\n",
      "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
      "Collecting future\n",
      "  Downloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "     -------------------------------------- 491.3/491.3 kB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: future, ffmpeg-python\n",
      "Successfully installed ffmpeg-python-0.2.0 future-1.0.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.2.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install ffmpeg-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"D:\\Tessaract\\Projects API\\twitter bot\\twitter_bot\\video generation\\ffmpeg\\bin\"\n",
    "\n",
    "def augment_video(video_path, times):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
    "\n",
    "    out = cv2.VideoWriter(\"first_output.mp4\", fourcc, fps, (width, height))\n",
    "\n",
    "    frames = []\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        frames.append(frame)\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "    if len(frames) < 7:  \n",
    "        print(\"Not enough frames to process smoothly.\")\n",
    "        return\n",
    "\n",
    "    frames = frames[5:]\n",
    "\n",
    "    sequence = []\n",
    "    for i in range(times):\n",
    "        if i % 2 == 0:\n",
    "            sequence.extend(frames)  \n",
    "        else:\n",
    "            sequence.extend(frames[::-1])\n",
    "\n",
    "    for frame in sequence:\n",
    "        out.write(frame)\n",
    "\n",
    "    out.release()\n",
    "    print(f\"Video saved as first_output.mp4 with {times} sequence(s).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48.82\n",
      "Video saved as first_output.mp4 with 49 sequence(s).\n",
      "Moviepy - Building video trimmed_output.mp4.\n",
      "MoviePy - Writing audio in trimmed_outputTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video trimmed_output.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready trimmed_output.mp4\n"
     ]
    }
   ],
   "source": [
    "# from moviepy import VideoFileClip, AudioFileClip\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip, AudioFileClip \n",
    "import math\n",
    "\n",
    "\n",
    "def increase_template_video_length(audio_path, video_path):\n",
    "    \n",
    "    audio = AudioFileClip(audio_path)\n",
    "    video = VideoFileClip(video_path)\n",
    "    \n",
    "    audio_length = audio.duration\n",
    "    video_length = video.duration\n",
    "    print(audio_length)\n",
    "    \n",
    "    # NOW, MAKING THE VIDEO EQUAL TO THE AUDIO LENGTH....\n",
    "    \n",
    "    if audio_length > video_length:\n",
    "        max_video_length = math.ceil(audio_length)\n",
    "        \n",
    "        augment_video(video_path, max_video_length)\n",
    "        \n",
    "        second_video = VideoFileClip('first_output.mp4')\n",
    "        second_video.audio = audio.subclip(0, audio.duration)\n",
    "\n",
    "        trimmed_video = second_video.subclip(0, audio_length)\n",
    "\n",
    "\n",
    "        trimmed_video.write_videofile(\"trimmed_output.mp4\", codec=\"libx264\", audio_codec=\"aac\")\n",
    "        \n",
    "\n",
    "\n",
    "# video_path = \"video_template.mp4\"\n",
    "video_path = \"video_template2.mp4\"\n",
    "audio_path = \"sample.mp3\"\n",
    "\n",
    "        \n",
    "increase_template_video_length(audio_path, video_path)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract text from the audio, to convert it to the text and plot within the cloud..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #OLD ONE.. SUBTITLES INTO THE CLOUD.....\n",
    "# import os\n",
    "# import cv2\n",
    "# import numpy as np\n",
    "# import moviepy as mp\n",
    "# import whisper\n",
    "\n",
    "# os.environ[\"PATH\"] += os.pathsep + r\"D:\\Tessaract\\Projects API\\twitter bot\\twitter_bot\\video_generation\\ffmpeg\\bin\"\n",
    "\n",
    "# def overlay_comic_cloud(video_path, output_path, subtitle_chunks, cloud_path):\n",
    "#     video = mp.VideoFileClip(video_path)\n",
    "#     cloud_img = cv2.imread(cloud_path, cv2.IMREAD_UNCHANGED)\n",
    "\n",
    "#     if cloud_img is None:\n",
    "#         raise ValueError(f\"Error loading comic cloud image from {cloud_path}\")\n",
    "\n",
    "#     cloud_h, cloud_w, _ = cloud_img.shape\n",
    "#     cloud_resized = cv2.resize(cloud_img, (int(cloud_w * 0.82), int(cloud_h * 0.9)))  \n",
    "\n",
    "#     # Define cloud position (move it further to the top-right)\n",
    "#     x_offset = video.w - cloud_resized.shape[1] - 2  # Push further right\n",
    "#     y_offset = 2  # Push further up\n",
    "\n",
    "#     # Extract bounding box for text placement\n",
    "#     cloud_box_x1 = x_offset + 42  # Left margin inside cloud\n",
    "#     cloud_box_x2 = x_offset + cloud_resized.shape[1] - 26  # Right margin\n",
    "#     cloud_box_y1 = y_offset + 58  # Top margin\n",
    "#     cloud_box_y2 = y_offset + cloud_resized.shape[0] - 43  # Bottom margin\n",
    "\n",
    "#     def add_overlay(frame):\n",
    "#         \"\"\" Overlay comic cloud and add subtitles dynamically. \"\"\"\n",
    "#         frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)  \n",
    "        \n",
    "#         for c in range(0, 3):\n",
    "#             alpha = cloud_resized[:, :, 3] / 255.0  # Extract alpha channel\n",
    "#             frame_bgr[y_offset:y_offset+cloud_resized.shape[0], x_offset:x_offset+cloud_resized.shape[1], c] = (\n",
    "#                 cloud_resized[:, :, c] * alpha +\n",
    "#                 frame_bgr[y_offset:y_offset+cloud_resized.shape[0], x_offset:x_offset+cloud_resized.shape[1], c] * (1.0 - alpha)\n",
    "#             )\n",
    "\n",
    "#         current_time = video.reader.pos / video.fps  \n",
    "#         current_text = \"\"\n",
    "#         for chunk in subtitle_chunks:\n",
    "#             if chunk['start'] <= current_time <= chunk['end']:\n",
    "#                 current_text = chunk['text']\n",
    "#                 break\n",
    "\n",
    "#         # Add subtitles inside the cloud\n",
    "#         if current_text:\n",
    "#             # font = cv2.FONT_HERSHEY_DUPLEX\n",
    "#             font = cv2.FONT_ITALIC\n",
    "#             font_scale = 0.5\n",
    "#             thickness = 1\n",
    "#             color = (0, 0, 0)  # Black text\n",
    "\n",
    "#             wrapped_text = wrap_text(current_text, font, font_scale, max_width=(cloud_box_x2 - cloud_box_x1))\n",
    "\n",
    "#             text_x = cloud_box_x1  # Left margin\n",
    "#             text_y = cloud_box_y1  # Start at the top inside the cloud\n",
    "\n",
    "#             for i, line in enumerate(wrapped_text):\n",
    "#                 line_y = text_y + i * 30  # Line spacing\n",
    "#                 if line_y < cloud_box_y2:  # Ensure text stays within cloud\n",
    "#                     cv2.putText(frame_bgr, line, (text_x, line_y), font, font_scale, color, thickness)\n",
    "\n",
    "#         return cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2RGB)  \n",
    "\n",
    "#     modified_video = video.image_transform(add_overlay)\n",
    "#     modified_video.write_videofile(output_path, codec=\"libx264\", fps=video.fps, audio_codec=\"aac\")\n",
    "\n",
    "# def wrap_text(text, font, font_scale, max_width):\n",
    "#     words = text.split()\n",
    "#     lines = []\n",
    "#     line = \"\"\n",
    "\n",
    "#     for word in words:\n",
    "#         test_line = line + \" \" + word if line else word\n",
    "#         text_size = cv2.getTextSize(test_line, font, font_scale, 2)[0]\n",
    "\n",
    "#         if text_size[0] <= max_width:\n",
    "#             line = test_line\n",
    "#         else:\n",
    "#             lines.append(line)\n",
    "#             line = word\n",
    "\n",
    "#     lines.append(line)  \n",
    "#     return lines\n",
    "\n",
    "\n",
    "# def extract_subtitles(audio_path, shift_time=-0.3):  # Shift subtitles back by 0.3s\n",
    "#     model = whisper.load_model(\"small\")\n",
    "#     result = model.transcribe(audio_path)\n",
    "#     subtitles = []\n",
    "\n",
    "#     for segment in result[\"segments\"]:\n",
    "#         start_time = max(0, segment[\"start\"] + shift_time)  # Prevent negative start time\n",
    "#         end_time = segment[\"end\"] + shift_time\n",
    "#         text = segment[\"text\"]\n",
    "\n",
    "#         mid_time = (start_time + end_time) / 2  \n",
    "#         words = text.split()\n",
    "\n",
    "#         if len(words) > 1:\n",
    "#             mid_index = len(words) // 2  \n",
    "#             first_half = \" \".join(words[:mid_index])\n",
    "#             second_half = \" \".join(words[mid_index:])\n",
    "\n",
    "#             subtitles.append({\n",
    "#                 \"start\": start_time,\n",
    "#                 \"end\": mid_time,\n",
    "#                 \"text\": first_half\n",
    "#             })\n",
    "#             subtitles.append({\n",
    "#                 \"start\": mid_time,\n",
    "#                 \"end\": end_time,\n",
    "#                 \"text\": second_half\n",
    "#             })\n",
    "#         else:\n",
    "#             subtitles.append({\n",
    "#                 \"start\": start_time,\n",
    "#                 \"end\": end_time,\n",
    "#                 \"text\": text\n",
    "#             })\n",
    "\n",
    "#     return subtitles\n",
    "\n",
    "\n",
    "\n",
    "# video_path = \"trimmed_output.mp4\"  \n",
    "# audio_path = \"sample.mp3\"\n",
    "# cloud_path = \"comic vectors/1_resized.png\"\n",
    "# output_video_path = \"final_output.mp4\"\n",
    "\n",
    "# subtitles = extract_subtitles(audio_path)\n",
    "# overlay_comic_cloud(video_path, output_video_path, subtitles, cloud_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Extract text from the audio, to convert it to the text and plot as subtitles..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Writing audio in sample.mp3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                      \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Building video video_with_subtitles.mp4.\n",
      "MoviePy - Writing audio in video_with_subtitlesTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video video_with_subtitles.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                 \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready video_with_subtitles.mp4\n",
      "Video processing complete. Saved as: video_with_subtitles.mp4\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import moviepy.editor as mp\n",
    "import whisper\n",
    "import textwrap\n",
    "from moviepy.video.io.VideoFileClip import VideoFileClip\n",
    "from moviepy.video.io.ImageSequenceClip import ImageSequenceClip\n",
    "from PIL import Image, ImageDraw, ImageFont\n",
    "\n",
    "\n",
    "os.environ[\"PATH\"] += os.pathsep + r\"D:\\Tessaract\\Projects API\\twitter bot\\twitter_bot\\video_generation\\ffmpeg\\bin\"\n",
    "\n",
    "\n",
    "video_path = \"trimmed_output.mp4\"\n",
    "video = VideoFileClip(video_path)\n",
    "fps = video.fps\n",
    "frame_width, frame_height = int(video.w), int(video.h)\n",
    "\n",
    "audio_path = \"sample.mp3\"\n",
    "video.audio.write_audiofile(audio_path)\n",
    "\n",
    "model = whisper.load_model(\"small\")\n",
    "result = model.transcribe(audio_path)\n",
    "\n",
    "subtitles = []\n",
    "shift_time = -0.1  # Adjust sync\n",
    "\n",
    "for segment in result[\"segments\"]:\n",
    "    start_time = max(0, segment[\"start\"] + shift_time)\n",
    "    end_time = segment[\"end\"] + shift_time\n",
    "    text = segment[\"text\"]\n",
    "    subtitles.append({\"start\": start_time, \"end\": end_time, \"text\": text})\n",
    "\n",
    "font_path = \"font/neue_pixel_sans/NeuePixelSans.ttf\"\n",
    "font_size = 30\n",
    "font = ImageFont.truetype(font_path, font_size)\n",
    "\n",
    "left_right_margin = 50  \n",
    "bottom_margin = 40  \n",
    "max_text_width = frame_width - (2 * left_right_margin)  \n",
    "\n",
    "def wrap_text(text, font, max_width):\n",
    "    words = text.split()\n",
    "    lines = []\n",
    "    current_line = \"\"\n",
    "\n",
    "    for word in words:\n",
    "        test_line = current_line + \" \" + word if current_line else word\n",
    "        text_size = font.getbbox(test_line)  # Get text width\n",
    "        text_width = text_size[2] - text_size[0]\n",
    "\n",
    "        if text_width <= max_width:\n",
    "            current_line = test_line\n",
    "        else:\n",
    "            lines.append(current_line)\n",
    "            current_line = word\n",
    "\n",
    "    if current_line:\n",
    "        lines.append(current_line)\n",
    "\n",
    "    return lines\n",
    "\n",
    "def draw_text(frame, text):\n",
    "    img_pil = Image.fromarray(frame)\n",
    "    draw = ImageDraw.Draw(img_pil)\n",
    "\n",
    "    wrapped_lines = wrap_text(text, font, max_text_width)    \n",
    "    total_text_height = sum(font.getbbox(line)[3] - font.getbbox(line)[1] for line in wrapped_lines)\n",
    "    text_y = frame_height - bottom_margin - total_text_height  \n",
    "\n",
    "    # shadow_offset = 3\n",
    "    # for line in wrapped_lines:\n",
    "    #     text_width = font.getbbox(line)[2] - font.getbbox(line)[0]\n",
    "    #     text_x = max(left_right_margin, (frame_width - text_width) // 2)  # Center horizontally if shorter\n",
    "    #     draw.text((text_x + shadow_offset, text_y + shadow_offset), line, font=font, fill=(0, 0, 0))  # Shadow\n",
    "    #     draw.text((text_x, text_y), line, font=font, fill=(255, 255, 255))  # White text\n",
    "    #     text_y += font.getbbox(line)[3] - font.getbbox(line)[1]  # Move to the next line\n",
    "\n",
    "\n",
    "    shadow_offset = 3\n",
    "    bold_offset = 0 \n",
    "\n",
    "    for line in wrapped_lines:\n",
    "        text_width = font.getbbox(line)[2] - font.getbbox(line)[0]\n",
    "        text_x = max(left_right_margin, (frame_width - text_width) // 2)  # Center horizontally if shorter\n",
    "        \n",
    "        # Shadow\n",
    "        draw.text((text_x + shadow_offset, text_y + shadow_offset), line, font=font, fill=(0, 0, 0))\n",
    "\n",
    "        # Fake Bold Effect: Draw text multiple times slightly shifted  Reason: PIL does not give option to bold text\n",
    "        for dx in range(-bold_offset, bold_offset + 1):\n",
    "            for dy in range(-bold_offset, bold_offset + 1):\n",
    "                draw.text((text_x + dx, text_y + dy), line, font=font, fill=(255, 255, 255))\n",
    "\n",
    "        text_y += font.getbbox(line)[3] - font.getbbox(line)[1]\n",
    "\n",
    "\n",
    "\n",
    "    return np.array(img_pil)\n",
    "\n",
    "# Process frames and overlay subtitles\n",
    "frames = []\n",
    "for i, frame in enumerate(video.iter_frames(fps=fps, dtype=\"uint8\")):\n",
    "    timestamp = i / fps\n",
    "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "    for subtitle in subtitles:\n",
    "        if subtitle[\"start\"] <= timestamp <= subtitle[\"end\"]:\n",
    "            frame_rgb = draw_text(frame_rgb, subtitle[\"text\"])\n",
    "    \n",
    "    frame_rgb = cv2.cvtColor(frame_rgb, cv2.COLOR_BGR2RGB)\n",
    "    frames.append(frame_rgb)\n",
    "\n",
    "output_video_path = \"video_with_subtitles.mp4\"\n",
    "clip = ImageSequenceClip(frames, fps=fps)\n",
    "clip = clip.set_audio(video.audio)  \n",
    "clip.write_videofile(output_video_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "\n",
    "print(\"Video processing complete. Saved as:\", output_video_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Building video final_video_with_music.mp4.\n",
      "MoviePy - Writing audio in final_video_with_musicTEMP_MPY_wvf_snd.mp4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                     \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MoviePy - Done.\n",
      "Moviepy - Writing video final_video_with_music.mp4\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Moviepy - Done !\n",
      "Moviepy - video ready final_video_with_music.mp4\n"
     ]
    }
   ],
   "source": [
    "from moviepy.editor import VideoFileClip, AudioFileClip, CompositeAudioClip\n",
    "from moviepy.audio.fx.all import volumex  \n",
    "\n",
    "def add_background_music(video_path, music_path, output_path, music_volume=0.05):\n",
    "    video = VideoFileClip(video_path)\n",
    "    bg_music = AudioFileClip(music_path).fx(volumex, music_volume) \n",
    "\n",
    "    bg_music = bg_music.set_duration(video.duration)  \n",
    "    final_audio = CompositeAudioClip([video.audio, bg_music])\n",
    "    final_video = video.set_audio(final_audio)\n",
    "    final_video.write_videofile(output_path, codec=\"libx264\", audio_codec=\"aac\")\n",
    "\n",
    "video_path = \"video_with_subtitles.mp4\"\n",
    "music_path = \"background_voices/Game music 1.mp3\"\n",
    "output_path = \"final_video_with_music.mp4\"\n",
    "\n",
    "add_background_music(video_path, music_path, output_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
